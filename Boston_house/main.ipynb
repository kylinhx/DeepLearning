{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 波士顿房价预测任务\n",
    "\n",
    "上一节我们初步认识了神经网络的基本概念（如神经元、多层连接、前向计算、计算图）和模型结构三要素（模型假设、评价函数和优化算法）。本节将以“波士顿房价预测”任务为例，向读者介绍使用Python语言和Numpy库来构建神经网络模型的思考过程和操作方法。\n",
    "\n",
    "波士顿房价预测是一个经典的机器学习任务，类似于程序员世界的“Hello World”。和大家对房价的普遍认知相同，波士顿地区的房价受诸多因素影响。该数据集统计了13种可能影响房价的因素和该类型房屋的均价，期望构建一个基于13个因素进行房价预测的模型，如 **图1** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/7a6bdaa3c11d41469d9301c09e1595074ee3acb37bf943959f7f9f28ef3bd251\" width=\"500\" hegiht=\"\" ></center>\n",
    "<center><br>图1：波士顿房价影响因素示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "对于预测问题，可以根据预测输出的类型是连续的实数值，还是离散的标签，区分为回归任务和分类任务。因为房价是一个连续值，所以房价预测显然是一个回归任务。下面我们尝试用最简单的线性回归模型解决这个问题，并用神经网络来实现这个模型。\n",
    "\n",
    "## 1.3.1 线性回归模型\n",
    "\n",
    "假设房价和各影响因素之间能够用线性关系来描述：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。其中，$w_j$和$b$分别表示该线性模型的权重和偏置。一维情况下，$w_j$ 和 $b$ 是直线的斜率和截距。\n",
    "\n",
    "线性回归模型使用均方误差作为（Mean Squared Error，MSE）损失函数（Loss），用以衡量预测房价和真实房价的差异，公式如下：\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^N(\\hat{Y_i} - {Y_i})^{2}$$\n",
    "\n",
    "------\n",
    "**思考：**\n",
    "\n",
    "为什么要以均方误差作为损失函数？即将模型在每个训练样本上的预测误差加和，来衡量整体样本的准确性。这是因为损失函数的设计不仅仅要考虑“合理性”（有物理意义），同样需要考虑“易解性”（易于求解），这个问题在后面的内容中会详细阐述。\n",
    "\n",
    "------\n",
    "\n",
    "## 1.3.2 线性回归模型的神经网络结构\n",
    "\n",
    "神经网络的标准结构中每个神经元由加权和与非线性变换构成，然后将多个神经元分层的摆放并连接形成神经网络。线性回归模型可以认为是神经网络模型的一种极简特例，是一个只有加权和、没有非线性变换的神经元（无需形成网络），如 **图2** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f9117a5a34d44b1eab85147e62b4e6295e485e48d79d4a03adaa14a447ffd230\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图2：线性回归模型的神经网络结构</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3 基于线性回归实现波士顿房价预测任务\n",
    "\n",
    "深度学习不仅实现了模型的端到端学习，还推动了人工智能进入工业大生产阶段，产生了标准化、自动化和模块化的通用框架。不同场景的深度学习模型具备一定的通用性，五个步骤即可完成模型的构建和训练，如 **图3** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/99d253f2759e40c8a5ee76a3d8243efc7ad965e4a293427c9d4677b23655e935\" width=\"800\" hegiht=\"\" ></center>\n",
    "<center><br>图3：构建神经网络/深度学习模型的基本步骤</br></center>\n",
    "<br></br>\n",
    "\n",
    "正是由于深度学习的建模和训练的过程存在通用性，即在构建不同的模型时，只有模型三要素不同，其它步骤基本一致，才产生了深度学习框架来加速建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3.1 数据处理\n",
    "\n",
    "数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "* 本教程中的代码都可以在AI Studio上直接运行，Print结果都是基于程序真实运行的结果。\n",
    "* 由于是真实案例，代码之间存在依赖关系，因此需要读者逐条、全部运行，否则会导致命令执行报错。\n",
    "\n",
    "------\n",
    "\n",
    "**（1）数据读取**\n",
    "\n",
    "通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:20.605786Z",
     "iopub.status.busy": "2022-09-23T06:53:20.605385Z",
     "iopub.status.idle": "2022-09-23T06:53:21.024069Z",
     "shell.execute_reply": "2022-09-23T06:53:21.023189Z",
     "shell.execute_reply.started": "2022-09-23T06:53:20.605762Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, ..., 3.969e+02, 7.880e+00,\n",
       "       1.190e+01])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入需要用到的package\n",
    "import numpy as np\n",
    "import json\n",
    "# 读入训练数据\n",
    "datafile = './work/housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（2）数据形状变换**\n",
    "\n",
    "由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个$x$（影响房价的特征）和一个$y$（该类型房屋的均价）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.026422Z",
     "iopub.status.busy": "2022-09-23T06:53:21.025680Z",
     "iopub.status.idle": "2022-09-23T06:53:21.033909Z",
     "shell.execute_reply": "2022-09-23T06:53:21.032586Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.026390Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... \n",
    "# 这里对原始数据做reshape，变成N x 14的形式\n",
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "feature_num = len(feature_names)\n",
    "data = data.reshape([data.shape[0] // feature_num, feature_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.035176Z",
     "iopub.status.busy": "2022-09-23T06:53:21.034921Z",
     "iopub.status.idle": "2022-09-23T06:53:21.040729Z",
     "shell.execute_reply": "2022-09-23T06:53:21.039916Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.035143Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "x = data[0]\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（3）数据集划分**\n",
    "\n",
    "将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。为什么要对数据集进行拆分，而不能直接应用于模型训练呢？这与学生时代的授课和考试关系比较类似，如 **图4** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/e6582546ee8c4a089610621db81e278cb20856fff81d496a807e7d570950da05\" width=\"600\" hegiht=\"\" ></center>\n",
    "<center><br>图4：训练集和测试集拆分的意义</br></center>\n",
    "<br></br>\n",
    "\n",
    "上学时总有一些自作聪明的同学，平时不认真学习，考试前临阵抱佛脚，将习题死记硬背下来，但是成绩往往并不好。因为学校期望学生掌握的是知识，而不仅仅是习题本身。另出新的考题，才能鼓励学生努力去掌握习题背后的原理。同样我们期望模型学习的是任务的本质规律，而不是训练数据本身，模型训练未使用的数据，才能更真实的评估模型的效果。\n",
    "\n",
    "在本案例中，我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有404个样本，每个样本含有13个特征和1个预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.043194Z",
     "iopub.status.busy": "2022-09-23T06:53:21.042812Z",
     "iopub.status.idle": "2022-09-23T06:53:21.048333Z",
     "shell.execute_reply": "2022-09-23T06:53:21.047581Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.043168Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "offset = int(data.shape[0] * ratio)\n",
    "training_data = data[:offset]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（4）数据归一化处理**\n",
    "\n",
    "对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.049984Z",
     "iopub.status.busy": "2022-09-23T06:53:21.049649Z",
     "iopub.status.idle": "2022-09-23T06:53:21.054892Z",
     "shell.execute_reply": "2022-09-23T06:53:21.054059Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.049962Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 计算train数据集的最大值，最小值\n",
    "maximums, minimums = \\\n",
    "                     training_data.max(axis=0), \\\n",
    "                     training_data.min(axis=0), \n",
    "# 对数据进行归一化处理\n",
    "for i in range(feature_num):\n",
    "    data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（5）封装成load data函数**\n",
    "\n",
    "将上述几个数据处理操作封装成`load data`函数，以便下一步模型的调用，实现方法如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.056582Z",
     "iopub.status.busy": "2022-09-23T06:53:21.055895Z",
     "iopub.status.idle": "2022-09-23T06:53:21.063270Z",
     "shell.execute_reply": "2022-09-23T06:53:21.062447Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.056558Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './work/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值\n",
    "    maximums, minimums = training_data.max(axis=0), \\\n",
    "                            training_data.min(axis=0)\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.064581Z",
     "iopub.status.busy": "2022-09-23T06:53:21.064254Z",
     "iopub.status.idle": "2022-09-23T06:53:21.072437Z",
     "shell.execute_reply": "2022-09-23T06:53:21.071511Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.064560Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "training_data, test_data = load_data()\n",
    "x = training_data[:, :-1]\n",
    "y = training_data[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.073878Z",
     "iopub.status.busy": "2022-09-23T06:53:21.073551Z",
     "iopub.status.idle": "2022-09-23T06:53:21.079025Z",
     "shell.execute_reply": "2022-09-23T06:53:21.077967Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.073855Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.18       0.07344184 0.         0.31481481 0.57750527\n",
      " 0.64160659 0.26920314 0.         0.22755741 0.28723404 1.\n",
      " 0.08967991]\n",
      "[0.42222222]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3.2 模型设计\n",
    "\n",
    "模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。\n",
    "\n",
    "如果将输入特征和输出预测值均以向量表示，输入特征$x$有13个向量，$y$有1个向量，那么参数权重的形状是$13\\times1$。假设我们以如下任意数字赋值参数做初始化：\n",
    "$$w=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.080619Z",
     "iopub.status.busy": "2022-09-23T06:53:21.080248Z",
     "iopub.status.idle": "2022-09-23T06:53:21.084678Z",
     "shell.execute_reply": "2022-09-23T06:53:21.083752Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.080594Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]\n",
    "w = np.array(w).reshape([13, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出第1条样本数据，观察样本的特征向量与参数向量相乘的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.088059Z",
     "iopub.status.busy": "2022-09-23T06:53:21.087740Z",
     "iopub.status.idle": "2022-09-23T06:53:21.092614Z",
     "shell.execute_reply": "2022-09-23T06:53:21.091834Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.088037Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69474855]\n"
     ]
    }
   ],
   "source": [
    "x1=x[0]\n",
    "t = np.dot(x1, w)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的线性回归公式，还需要初始化偏移量$b$，同样随意赋初值-0.2。那么，线性回归模型的完整输出是$z=t+b$，这个从特征和参数计算输出值的过程称为“前向计算”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.094001Z",
     "iopub.status.busy": "2022-09-23T06:53:21.093762Z",
     "iopub.status.idle": "2022-09-23T06:53:21.098635Z",
     "shell.execute_reply": "2022-09-23T06:53:21.097752Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.093981Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49474855]\n"
     ]
    }
   ],
   "source": [
    "b = -0.2\n",
    "z = t + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述计算预测输出的过程以“类和对象”的方式来描述，类成员变量有参数$w$和$b$。通过写一个`forward`函数（代表“前向计算”）完成上述从特征和参数到输出预测值的计算过程，代码实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.100083Z",
     "iopub.status.busy": "2022-09-23T06:53:21.099811Z",
     "iopub.status.idle": "2022-09-23T06:53:21.104982Z",
     "shell.execute_reply": "2022-09-23T06:53:21.104202Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.100053Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，\n",
    "        # 此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Network类的定义，模型的计算过程如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.107136Z",
     "iopub.status.busy": "2022-09-23T06:53:21.106631Z",
     "iopub.status.idle": "2022-09-23T06:53:21.113039Z",
     "shell.execute_reply": "2022-09-23T06:53:21.112013Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.107102Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.39362982]\n"
     ]
    }
   ],
   "source": [
    "net = Network(13)\n",
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z = net.forward(x1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3.3 训练配置\n",
    "\n",
    "模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。\n",
    "\n",
    "通过模型计算$x_1$表示的影响因素所对应的房价应该是$z$, 但实际数据告诉我们房价是$y$。这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标，公式为\n",
    "\n",
    "$$Loss = (y - z)^2$$\n",
    "\n",
    "上式中的$Loss$通常也被称作损失函数，它是衡量模型好坏的指标。在回归问题中常用均方误差作为损失函数，而在分类问题中常用采用交叉熵（Cross-Entropy）作为损失函数，在后续的章节中会更详细的介绍。对其中任意一个样本计算损失函数值的代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.114678Z",
     "iopub.status.busy": "2022-09-23T06:53:21.114319Z",
     "iopub.status.idle": "2022-09-23T06:53:21.120285Z",
     "shell.execute_reply": "2022-09-23T06:53:21.119420Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.114646Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.88644793]\n"
     ]
    }
   ],
   "source": [
    "Loss = (y1 - z)*(y1 - z)\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为计算损失函数时需要把每个样本的损失函数值都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数$N$。公式为\n",
    "\n",
    "$$Loss= \\frac{1}{N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "\n",
    "在Network类下面添加损失函数的代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.121898Z",
     "iopub.status.busy": "2022-09-23T06:53:21.121553Z",
     "iopub.status.idle": "2022-09-23T06:53:21.127266Z",
     "shell.execute_reply": "2022-09-23T06:53:21.126536Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.121875Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        cost = error * error\n",
    "        cost = np.mean(cost)\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用定义的Network类，可以方便的计算预测值和损失函数。需要注意的是，类中的变量$\\mathbf{x}$, $w$，$b$, $z$, $error$等均是向量。以变量$\\mathbf{x}$为例，共有两个维度，一个代表特征数量（值为13），一个代表样本数量，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.128765Z",
     "iopub.status.busy": "2022-09-23T06:53:21.128306Z",
     "iopub.status.idle": "2022-09-23T06:53:21.135448Z",
     "shell.execute_reply": "2022-09-23T06:53:21.134476Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.128741Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:  [[2.39362982]\n",
      " [2.46752393]\n",
      " [2.02483479]]\n",
      "loss: 3.384496992612791\n"
     ]
    }
   ],
   "source": [
    "net = Network(13)\n",
    "# 此处可以一次性计算多个样本的预测值和损失函数\n",
    "x1 = x[0:3]\n",
    "y1 = y[0:3]\n",
    "z = net.forward(x1)\n",
    "print('predict: ', z)\n",
    "loss = net.loss(z, y1)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3.4 训练过程\n",
    "\n",
    "上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数$w$和$b$的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数尽可能的小，也就是说找到一个参数解$w$和$b$，使得损失函数取得极小值。\n",
    "\n",
    "我们先做一个小测试：如 **图5** 所示，基于微积分知识，求一条曲线在某个点的斜率等于函数在该点的导数值。那么大家思考下，当处于曲线的极值点时，该点的斜率是多少？\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/94f0437e6a454a0682f3b831c96a62bdaf40898af25145ec9b5b50bc80391f5c\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图5：曲线斜率等于导数值</br></center>\n",
    "<br></br>\n",
    "\n",
    "这个问题并不难回答，处于曲线极值点时的斜率为0，即函数在极值点的导数为0。那么，让损失函数取极小值的$w$和$b$应该是下述方程组的解：\n",
    "$$\\frac{\\partial{L}}{\\partial{w}}=0$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}=0$$\n",
    "\n",
    "将样本数据$(x, y)$带入上面的方程组中即可求解出$w$和$b$的值，但是这种方法只对线性回归这样简单的任务有效。如果模型中含有非线性变换，或者损失函数不是均方差这种简单的形式，则很难通过上式求解。为了解决这个问题，下面我们将引入更加普适的数值求解方法：梯度下降法。\n",
    "\n",
    "**（1）梯度下降法**\n",
    "\n",
    "在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数，这种函数在密码学中有大量的应用。密码锁的特点是可以迅速判断一个密钥是否是正确的(已知$x$，求$y$很容易)，但是即使获取到密码锁系统，无法破解出正确的密钥是什么（已知$y$，求$x$很难）。神经网络模型的损失函数就是这样的单向函数，反向求解并不容易。\n",
    "\n",
    "这种情况特别类似于一位想从山峰走到坡谷的盲人，他看不见坡谷在哪（无法逆向求解出$Loss$导数为0时的参数值），但可以伸脚探索身边的坡度（当前点的导数值，也称为梯度）。那么，求解Loss函数最小值可以这样实现：从当前的参数取值，一步步的按照下坡的方向下降，直到走到最低点。这种方法笔者称它为“盲人下坡法”。哦不，有个更正式的说法“梯度下降法（Gradient Descent，GD）”。\n",
    "\n",
    "训练的关键是找到一组$(w, b)$，使得损失函数$L$取极小值。我们先看一下损失函数$L$只随两个参数$w_5$、$w_9$变化时的简单情形，启发下寻解的思路。\n",
    "$$L=L(w_5, w_9)$$\n",
    "这里我们将$w_0, w_1, ..., w_{12}$中除$w_5, w_9$之外的参数和$b$都固定下来，可以用图画出$L(w_5, w_9)$的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:21.136813Z",
     "iopub.status.busy": "2022-09-23T06:53:21.136539Z",
     "iopub.status.idle": "2022-09-23T06:53:28.519514Z",
     "shell.execute_reply": "2022-09-23T06:53:28.518618Z",
     "shell.execute_reply.started": "2022-09-23T06:53:21.136789Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = Network(13)\n",
    "losses = []\n",
    "#只画出参数w5和w9在区间[-160, 160]的曲线部分，以及包含损失函数的极值\n",
    "w5 = np.arange(-160.0, 160.0, 1.0)\n",
    "w9 = np.arange(-160.0, 160.0, 1.0)\n",
    "losses = np.zeros([len(w5), len(w9)])\n",
    "\n",
    "#计算设定区域内每个参数取值所对应的Loss\n",
    "for i in range(len(w5)):\n",
    "    for j in range(len(w9)):\n",
    "        net.w[5] = w5[i]\n",
    "        net.w[9] = w9[j]\n",
    "        z = net.forward(x)\n",
    "        loss = net.loss(z, y)\n",
    "        losses[i, j] = loss\n",
    "\n",
    "#使用matplotlib将两个变量和对应的Loss作3D图\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "w5, w9 = np.meshgrid(w5, w9)\n",
    "\n",
    "ax.plot_surface(w5, w9, losses, rstride=1, cstride=1, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于这种简单情形，我们利用上面的程序，可以在三维空间中画出损失函数随参数变化的曲面图。从图中可以看出有些区域的函数值明显比周围的点小。\n",
    "\n",
    "需要说明的是：为什么这里我们选择$w_5$和$w_9$来画图？这是因为选择这两个参数的时候，可比较直观的从损失函数的曲面图上发现极值点的存在。其他参数组合，从图形上观测损失函数的极值点不够直观。\n",
    "\n",
    "观察上述曲线呈现出“圆滑”的坡度，这正是我们选择以均方误差作为损失函数的原因之一。**图6** 呈现了只有一个参数维度时，均方误差和绝对值误差（只将每个样本的误差累加，不做平方处理）的损失函数曲线图。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/99487dca6520441db5073d1c154b5d2fb1174b5cf4d946c29f9d80a209bc2687\" width=\"700\" hegiht=\"40\" ></center>\n",
    "<center><br>图6：均方误差和绝对值误差损失函数曲线图</br></center>\n",
    "<br></br>\n",
    "\n",
    "由此可见，均方误差表现的“圆滑”的坡度有两个好处：\n",
    "\n",
    "* 曲线的最低点是可导的。\n",
    "* 越接近最低点，曲线的坡度逐渐放缓，有助于通过当前的梯度来判断接近最低点的程度（是否逐渐减少步长，以免错过最低点）。\n",
    "\n",
    "而绝对值误差是不具备这两个特性的，这也是损失函数的设计不仅仅要考虑“合理性”，还要追求“易解性”的原因。\n",
    "\n",
    "现在我们要找出一组$[w_5, w_9]$的值，使得损失函数最小，实现梯度下降法的方案如下：\n",
    "\n",
    "- 步骤1：随机的选一组初始值，例如：$[w_5, w_9] = [-100.0, -100.0]$\n",
    "- 步骤2：选取下一个点$[w_5^{'} , w_9^{'}]$，使得$L(w_5^{'} , w_9^{'}) < L(w_5, w_9)$\n",
    "- 步骤3：重复步骤2，直到损失函数几乎不再下降。\n",
    "\n",
    "如何选择$[w_5^{'} , w_9^{'}]$是至关重要的，第一要保证$L$是下降的，第二要使得下降的趋势尽可能的快。微积分的基础知识告诉我们，沿着梯度的反方向，是函数值下降最快的方向，如 **图7** 所示。简单理解，函数在某一个点的梯度方向是曲线斜率最大的方向，但梯度方向是向上的，所以下降最快的是梯度的反方向。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5f8322f6172542dab0f78684b70efe45d819895332af4cabb7c536217ab0bb26\" width=\"400\" hegiht=\"40\" ></center>\n",
    "<center><br>图7：梯度下降方向示意图</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（2）计算梯度**\n",
    "\n",
    "上面我们讲过了损失函数的计算方法，这里稍微改写，为了使梯度计算更加简洁，引入因子$\\frac{1}{2}$，定义损失函数如下：\n",
    "\n",
    "$$L= \\frac{1}{2N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "\n",
    "其中$z_i$是网络对第$i$个样本的预测值：\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{12}{x_i^{j}\\cdot w_j} + b$$\n",
    "\n",
    "梯度的定义：\n",
    "\n",
    "$$𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡 = (\\frac{\\partial{L}}{\\partial{w_0}},\\frac{\\partial{L}}{\\partial{w_1}}, ... ,\\frac{\\partial{L}}{\\partial{w_{12}}} ,\\frac{\\partial{L}}{\\partial{b}})$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{b}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)}$$\n",
    "\n",
    "从导数的计算过程可以看出，因子$\\frac{1}{2}$被消掉了，这是因为二次函数求导的时候会产生因子$2$，这也是我们将损失函数改写的原因。\n",
    "\n",
    "下面我们考虑只有一个样本的情况下，计算梯度：\n",
    "\n",
    "$$L= \\frac{1}{2}{(y_i - z_i)^2}$$\n",
    "\n",
    "$$z_1 = {x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b$$\n",
    "\n",
    "可以计算出：\n",
    "\n",
    "$$L= \\frac{1}{2}{({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)^2}$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_0}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_12} + b - y_1)\\cdot x_1^{0}=({z_1} - {y_1})\\cdot x_1^{0}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)\\cdot 1 = ({z_1} - {y_1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "可以通过具体的程序查看每个变量的数据和维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.521331Z",
     "iopub.status.busy": "2022-09-23T06:53:28.520805Z",
     "iopub.status.idle": "2022-09-23T06:53:28.528216Z",
     "shell.execute_reply": "2022-09-23T06:53:28.527368Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.521304Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 [0.         0.18       0.07344184 0.         0.31481481 0.57750527\n",
      " 0.64160659 0.26920314 0.         0.22755741 0.28723404 1.\n",
      " 0.08967991], shape (13,)\n",
      "y1 [0.42222222], shape (1,)\n",
      "z1 [130.86954441], shape (1,)\n"
     ]
    }
   ],
   "source": [
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z1 = net.forward(x1)\n",
    "print('x1 {}, shape {}'.format(x1, x1.shape))\n",
    "print('y1 {}, shape {}'.format(y1, y1.shape))\n",
    "print('z1 {}, shape {}'.format(z1, z1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按上面的公式，当只有一个样本时，可以计算某个$w_j$，比如$w_0$的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.529718Z",
     "iopub.status.busy": "2022-09-23T06:53:28.529324Z",
     "iopub.status.idle": "2022-09-23T06:53:28.534458Z",
     "shell.execute_reply": "2022-09-23T06:53:28.533665Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.529680Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w0 [0.]\n"
     ]
    }
   ],
   "source": [
    "gradient_w0 = (z1 - y1) * x1[0]\n",
    "print('gradient_w0 {}'.format(gradient_w0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样我们可以计算$w_1$的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.536052Z",
     "iopub.status.busy": "2022-09-23T06:53:28.535536Z",
     "iopub.status.idle": "2022-09-23T06:53:28.540875Z",
     "shell.execute_reply": "2022-09-23T06:53:28.539967Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.536023Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w1 [23.48051799]\n"
     ]
    }
   ],
   "source": [
    "gradient_w1 = (z1 - y1) * x1[1]\n",
    "print('gradient_w1 {}'.format(gradient_w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依次计算$w_2$的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.542552Z",
     "iopub.status.busy": "2022-09-23T06:53:28.541963Z",
     "iopub.status.idle": "2022-09-23T06:53:28.547215Z",
     "shell.execute_reply": "2022-09-23T06:53:28.546293Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.542522Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w2 [9.58029163]\n"
     ]
    }
   ],
   "source": [
    "gradient_w2= (z1 - y1) * x1[2]\n",
    "print('gradient_w2 {}'.format(gradient_w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聪明的读者可能已经想到，写一个for循环即可计算从$w_0$到$w_{12}$的所有权重的梯度，该方法读者可以自行实现。\n",
    "\n",
    "**（3）使用Numpy进行梯度计算**\n",
    "\n",
    "基于Numpy广播机制（对向量和矩阵计算如同对1个单一变量计算一样），可以更快速的实现梯度计算。计算梯度的代码中直接用$(z_1 - y_1) \\cdot x_1$，得到的是一个13维的向量，每个分量分别代表该维度的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.549134Z",
     "iopub.status.busy": "2022-09-23T06:53:28.548474Z",
     "iopub.status.idle": "2022-09-23T06:53:28.553714Z",
     "shell.execute_reply": "2022-09-23T06:53:28.552894Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.549106Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample1 [  0.          23.48051799   9.58029163   0.          41.06674958\n",
      "  75.33401592  83.69586171  35.11682862   0.          29.68425495\n",
      "  37.46891169 130.44732219  11.69850434], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = (z1 - y1) * x1\n",
    "print('gradient_w_by_sample1 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入数据中有多个样本，每个样本都对梯度有贡献。如上代码计算了只有样本1时的梯度值，同样的计算方法也可以计算样本2和样本3对梯度的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.555170Z",
     "iopub.status.busy": "2022-09-23T06:53:28.554902Z",
     "iopub.status.idle": "2022-09-23T06:53:28.560723Z",
     "shell.execute_reply": "2022-09-23T06:53:28.559873Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.555147Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample2 [2.54738434e-02 0.00000000e+00 2.83333765e+01 0.00000000e+00\n",
      " 1.86624242e+01 5.91703008e+01 8.45121992e+01 3.76793284e+01\n",
      " 4.69458498e+00 1.23980167e+01 5.97311025e+01 1.07975454e+02\n",
      " 2.20777626e+01], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "x2 = x[1]\n",
    "y2 = y[1]\n",
    "z2 = net.forward(x2)\n",
    "gradient_w = (z2 - y2) * x2\n",
    "print('gradient_w_by_sample2 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.562020Z",
     "iopub.status.busy": "2022-09-23T06:53:28.561757Z",
     "iopub.status.idle": "2022-09-23T06:53:28.567665Z",
     "shell.execute_reply": "2022-09-23T06:53:28.566960Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.561996Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample3 [3.07963708e-02 0.00000000e+00 3.42860463e+01 0.00000000e+00\n",
      " 2.25832858e+01 9.07287666e+01 7.83155260e+01 4.55955257e+01\n",
      " 5.68088867e+00 1.50027645e+01 7.22802431e+01 1.29029688e+02\n",
      " 8.29246719e+00], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "x3 = x[2]\n",
    "y3 = y[2]\n",
    "z3 = net.forward(x3)\n",
    "gradient_w = (z3 - y3) * x3\n",
    "print('gradient_w_by_sample3 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可能有的读者再次想到可以使用for循环把每个样本对梯度的贡献都计算出来，然后再作平均。但是我们不需要这么做，仍然可以使用Numpy的矩阵操作来简化运算，如3个样本的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.569087Z",
     "iopub.status.busy": "2022-09-23T06:53:28.568642Z",
     "iopub.status.idle": "2022-09-23T06:53:28.575469Z",
     "shell.execute_reply": "2022-09-23T06:53:28.574775Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.569063Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[0.00000000e+00 1.80000000e-01 7.34418420e-02 0.00000000e+00\n",
      "  3.14814815e-01 5.77505269e-01 6.41606591e-01 2.69203139e-01\n",
      "  0.00000000e+00 2.27557411e-01 2.87234043e-01 1.00000000e+00\n",
      "  8.96799117e-02]\n",
      " [2.35922539e-04 0.00000000e+00 2.62405717e-01 0.00000000e+00\n",
      "  1.72839506e-01 5.47997701e-01 7.82698249e-01 3.48961980e-01\n",
      "  4.34782609e-02 1.14822547e-01 5.53191489e-01 1.00000000e+00\n",
      "  2.04470199e-01]\n",
      " [2.35697744e-04 0.00000000e+00 2.62405717e-01 0.00000000e+00\n",
      "  1.72839506e-01 6.94385898e-01 5.99382080e-01 3.48961980e-01\n",
      "  4.34782609e-02 1.14822547e-01 5.53191489e-01 9.87519166e-01\n",
      "  6.34657837e-02]], shape (3, 13)\n",
      "y [[0.42222222]\n",
      " [0.36888889]\n",
      " [0.66      ]], shape (3, 1)\n",
      "z [[130.86954441]\n",
      " [108.34434338]\n",
      " [131.3204395 ]], shape (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# 注意这里是一次取出3个样本的数据，不是取出第3个样本\n",
    "x3samples = x[0:3]\n",
    "y3samples = y[0:3]\n",
    "z3samples = net.forward(x3samples)\n",
    "\n",
    "print('x {}, shape {}'.format(x3samples, x3samples.shape))\n",
    "print('y {}, shape {}'.format(y3samples, y3samples.shape))\n",
    "print('z {}, shape {}'.format(z3samples, z3samples.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的x3samples, y3samples, z3samples的第一维大小均为3，表示有3个样本。下面计算这3个样本对梯度的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.577208Z",
     "iopub.status.busy": "2022-09-23T06:53:28.576522Z",
     "iopub.status.idle": "2022-09-23T06:53:28.581605Z",
     "shell.execute_reply": "2022-09-23T06:53:28.580919Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.577181Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w [[0.00000000e+00 2.34805180e+01 9.58029163e+00 0.00000000e+00\n",
      "  4.10667496e+01 7.53340159e+01 8.36958617e+01 3.51168286e+01\n",
      "  0.00000000e+00 2.96842549e+01 3.74689117e+01 1.30447322e+02\n",
      "  1.16985043e+01]\n",
      " [2.54738434e-02 0.00000000e+00 2.83333765e+01 0.00000000e+00\n",
      "  1.86624242e+01 5.91703008e+01 8.45121992e+01 3.76793284e+01\n",
      "  4.69458498e+00 1.23980167e+01 5.97311025e+01 1.07975454e+02\n",
      "  2.20777626e+01]\n",
      " [3.07963708e-02 0.00000000e+00 3.42860463e+01 0.00000000e+00\n",
      "  2.25832858e+01 9.07287666e+01 7.83155260e+01 4.55955257e+01\n",
      "  5.68088867e+00 1.50027645e+01 7.22802431e+01 1.29029688e+02\n",
      "  8.29246719e+00]], gradient.shape (3, 13)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = (z3samples - y3samples) * x3samples\n",
    "print('gradient_w {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处可见，计算梯度`gradient_w`的维度是$3 \\times 13$，并且其第1行与上面第1个样本计算的梯度gradient_w_by_sample1一致，第2行与上面第2个样本计算的梯度gradient_w_by_sample2一致，第3行与上面第3个样本计算的梯度gradient_w_by_sample3一致。这里使用矩阵操作，可以更加方便的对3个样本分别计算各自对梯度的贡献。\n",
    "\n",
    "那么对于有N个样本的情形，我们可以直接使用如下方式计算出所有样本对梯度的贡献，这就是使用Numpy库广播功能带来的便捷。\n",
    "小结一下这里使用Numpy库的广播功能：\n",
    "- 一方面可以扩展参数的维度，代替for循环来计算1个样本对从$w_{0}$到$w_{12}$的所有参数的梯度。\n",
    "- 另一方面可以扩展样本的维度，代替for循环来计算样本0到样本403对参数的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.583697Z",
     "iopub.status.busy": "2022-09-23T06:53:28.582732Z",
     "iopub.status.idle": "2022-09-23T06:53:28.589061Z",
     "shell.execute_reply": "2022-09-23T06:53:28.588341Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.583670Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w shape (404, 13)\n",
      "[[0.00000000e+00 2.34805180e+01 9.58029163e+00 ... 3.74689117e+01\n",
      "  1.30447322e+02 1.16985043e+01]\n",
      " [2.54738434e-02 0.00000000e+00 2.83333765e+01 ... 5.97311025e+01\n",
      "  1.07975454e+02 2.20777626e+01]\n",
      " [3.07963708e-02 0.00000000e+00 3.42860463e+01 ... 7.22802431e+01\n",
      "  1.29029688e+02 8.29246719e+00]\n",
      " ...\n",
      " [3.97706874e+01 0.00000000e+00 1.74130673e+02 ... 2.01043762e+02\n",
      "  2.48659390e+02 1.27554582e+02]\n",
      " [2.69696515e+01 0.00000000e+00 1.75225687e+02 ... 2.02308019e+02\n",
      "  2.34270491e+02 1.28287658e+02]\n",
      " [6.08972123e+01 0.00000000e+00 1.53017134e+02 ... 1.76666981e+02\n",
      "  2.18509161e+02 1.08772220e+02]]\n"
     ]
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "print('gradient_w shape {}'.format(gradient_w.shape))\n",
    "print(gradient_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面gradient_w的每一行代表了一个样本对梯度的贡献。根据梯度的计算公式，总梯度是对每个样本对梯度贡献的平均值。\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}}$$\n",
    "\n",
    "我们也可以使用Numpy的均值函数来完成此过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.595128Z",
     "iopub.status.busy": "2022-09-23T06:53:28.594767Z",
     "iopub.status.idle": "2022-09-23T06:53:28.601381Z",
     "shell.execute_reply": "2022-09-23T06:53:28.600574Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.595102Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w  (13,)\n",
      "w  (13, 1)\n",
      "[  4.6555403   19.35268996  55.88081118  14.00266972  47.98588869\n",
      "  76.87210821  94.8555119   36.07579608  45.44575958  59.65733292\n",
      "  83.65114918 134.80387478  38.93998153]\n",
      "[[ 1.76405235e+00]\n",
      " [ 4.00157208e-01]\n",
      " [ 9.78737984e-01]\n",
      " [ 2.24089320e+00]\n",
      " [ 1.86755799e+00]\n",
      " [ 1.59000000e+02]\n",
      " [ 9.50088418e-01]\n",
      " [-1.51357208e-01]\n",
      " [-1.03218852e-01]\n",
      " [ 1.59000000e+02]\n",
      " [ 1.44043571e-01]\n",
      " [ 1.45427351e+00]\n",
      " [ 7.61037725e-01]]\n"
     ]
    }
   ],
   "source": [
    "# axis = 0 表示把每一行做相加然后再除以总的行数\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "print('gradient_w ', gradient_w.shape)\n",
    "print('w ', net.w.shape)\n",
    "print(gradient_w)\n",
    "print(net.w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用Numpy的矩阵操作方便地完成了gradient的计算，但引入了一个问题，`gradient_w`的形状是(13,)，而$w$的维度是(13, 1)。导致该问题的原因是使用`np.mean`函数时消除了第0维。为了加减乘除等计算方便，`gradient_w`和$w$必须保持一致的形状。因此我们将`gradient_w`的维度也设置为(13,1)，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.602895Z",
     "iopub.status.busy": "2022-09-23T06:53:28.602625Z",
     "iopub.status.idle": "2022-09-23T06:53:28.607090Z",
     "shell.execute_reply": "2022-09-23T06:53:28.606183Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.602871Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w shape (13, 1)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = gradient_w[:, np.newaxis]\n",
    "print('gradient_w shape', gradient_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合上面的剖析，计算梯度的代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.608548Z",
     "iopub.status.busy": "2022-09-23T06:53:28.608100Z",
     "iopub.status.idle": "2022-09-23T06:53:28.615697Z",
     "shell.execute_reply": "2022-09-23T06:53:28.614624Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.608522Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.6555403 ],\n",
       "       [ 19.35268996],\n",
       "       [ 55.88081118],\n",
       "       [ 14.00266972],\n",
       "       [ 47.98588869],\n",
       "       [ 76.87210821],\n",
       "       [ 94.8555119 ],\n",
       "       [ 36.07579608],\n",
       "       [ 45.44575958],\n",
       "       [ 59.65733292],\n",
       "       [ 83.65114918],\n",
       "       [134.80387478],\n",
       "       [ 38.93998153]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "gradient_w = gradient_w[:, np.newaxis]\n",
    "gradient_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码非常简洁地完成了$w$的梯度计算。同样，计算$b$的梯度的代码也是类似的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.617132Z",
     "iopub.status.busy": "2022-09-23T06:53:28.616731Z",
     "iopub.status.idle": "2022-09-23T06:53:28.621947Z",
     "shell.execute_reply": "2022-09-23T06:53:28.621226Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.617108Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.50289323156107"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_b = (z - y)\n",
    "gradient_b = np.mean(gradient_b)\n",
    "# 此处b是一个数值，所以可以直接用np.mean得到一个标量\n",
    "gradient_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面计算$w$和$b$的梯度的过程，写成Network类的`gradient`函数，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.623490Z",
     "iopub.status.busy": "2022-09-23T06:53:28.623066Z",
     "iopub.status.idle": "2022-09-23T06:53:28.630451Z",
     "shell.execute_reply": "2022-09-23T06:53:28.629685Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.623465Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)\n",
    "        \n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.631972Z",
     "iopub.status.busy": "2022-09-23T06:53:28.631419Z",
     "iopub.status.idle": "2022-09-23T06:53:28.639492Z",
     "shell.execute_reply": "2022-09-23T06:53:28.638547Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.631947Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-100.0, -100.0], loss 7873.345739941161\n",
      "gradient [-45.87968288123223, -35.50236884482904]\n"
     ]
    }
   ],
   "source": [
    "# 调用上面定义的gradient函数，计算梯度\n",
    "# 初始化网络\n",
    "net = Network(13)\n",
    "# 设置[w5, w9] = [-100., -100.]\n",
    "net.w[5] = -100.0\n",
    "net.w[9] = -100.0\n",
    "\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（4）确定损失函数更小的点**\n",
    "\n",
    "下面我们开始研究更新梯度的方法。首先沿着梯度的反方向移动一小步，找到下一个点P1，观察损失函数的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.640951Z",
     "iopub.status.busy": "2022-09-23T06:53:28.640676Z",
     "iopub.status.idle": "2022-09-23T06:53:28.659379Z",
     "shell.execute_reply": "2022-09-23T06:53:28.658510Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.640927Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-95.41203171187678, -96.4497631155171], loss 7214.694816482369\n",
      "gradient [-43.883932999069096, -34.019273908495926]\n"
     ]
    }
   ],
   "source": [
    "# 在[w5, w9]平面上，沿着梯度的反方向移动到下一个点P1\n",
    "# 定义移动步长 eta\n",
    "eta = 0.1\n",
    "# 更新参数w5和w9\n",
    "net.w[5] = net.w[5] - eta * gradient_w5\n",
    "net.w[9] = net.w[9] - eta * gradient_w9\n",
    "# 重新计算z和loss\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面的代码，可以发现沿着梯度反方向走一小步，下一个点的损失函数的确减少了。感兴趣的话，大家可以尝试不停的点击上面的代码块，观察损失函数是否一直在变小。\n",
    "\n",
    "在上述代码中，每次更新参数使用的语句：\n",
    "`net.w[5] = net.w[5] - eta * gradient_w5`\n",
    "\n",
    "* 相减：参数需要向梯度的反方向移动。\n",
    "* eta：控制每次参数值沿着梯度反方向变动的大小，即每次移动的步长，又称为学习率。\n",
    "\n",
    "大家可以思考下，为什么之前我们要做输入特征的归一化，保持尺度一致？这是为了让统一的步长更加合适。\n",
    "\n",
    "如 **图8** 所示，特征输入归一化后，不同参数输出的Loss是一个比较规整的曲线，学习率可以设置成统一的值 ；特征输入未归一化时，不同特征对应的参数所需的步长不一致，尺度较大的参数需要大步长，尺寸较小的参数需要小步长，导致无法设置统一的学习率。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/903f552bc55b4a5eba71caa7dd86fd2d7b71b8ebb6cb4500a5f5711f465707f3\" width=\"300\" hegiht=\"40\" ></center>\n",
    "<center><br>图8：未归一化的特征，会导致不同特征维度的理想步长不同</br></center>\n",
    "<br></br>\n",
    "\n",
    "**（5）代码封装Train函数**\n",
    "\n",
    "将上面的循环计算过程封装在`train`和`update`函数中，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:28.660605Z",
     "iopub.status.busy": "2022-09-23T06:53:28.660373Z",
     "iopub.status.idle": "2022-09-23T06:53:29.100464Z",
     "shell.execute_reply": "2022-09-23T06:53:29.099406Z",
     "shell.execute_reply.started": "2022-09-23T06:53:28.660585Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, point [-99.54120317118768, -99.64497631155172], loss 7873.345739941161\n",
      "iter 50, point [-78.9761810944732, -83.65939206734069], loss 5131.480704109405\n",
      "iter 100, point [-62.4493631356931, -70.67918223434114], loss 3346.754494352463\n",
      "iter 150, point [-49.17799206644332, -60.12620415441553], loss 2184.906016270654\n",
      "iter 200, point [-38.53070194231174, -51.533984751788346], loss 1428.4172504483342\n",
      "iter 250, point [-29.998249130283174, -44.52613603923428], loss 935.7392894242679\n",
      "iter 300, point [-23.169901624519575, -38.79894318028118], loss 614.7592258739251\n",
      "iter 350, point [-17.71439280083778, -34.10731848231335], loss 405.53408184471505\n",
      "iter 400, point [-13.364557220746388, -30.253470630210863], loss 269.0551396220099\n",
      "iter 450, point [-9.904936677384967, -27.077764259976597], loss 179.9364750604248\n",
      "iter 500, point [-7.161782280775628, -24.451346444229817], loss 121.65711285489998\n",
      "iter 550, point [-4.994989383373879, -22.270198517465555], loss 83.46491706360901\n",
      "iter 600, point [-3.2915916915280783, -20.450337700789422], loss 58.36183370758033\n",
      "iter 650, point [-1.9605131425212885, -18.923946252536773], loss 41.792808952534\n",
      "iter 700, point [-0.9283343968114077, -17.636248840494844], loss 30.792614998570482\n",
      "iter 750, point [-0.13587780041668718, -16.542993494033716], loss 23.43065354742935\n",
      "iter 800, point [0.4645474092373408, -15.60841945615185], loss 18.449664464381506\n",
      "iter 850, point [0.9113672926170796, -14.803617811655524], loss 15.030615923519784\n",
      "iter 900, point [1.2355357562745004, -14.105208963393421], loss 12.639705730905764\n",
      "iter 950, point [1.4619805189121953, -13.494275706622066], loss 10.928795653764196\n",
      "iter 1000, point [1.6107694974712377, -12.955502492189021], loss 9.670616807081698\n",
      "iter 1050, point [1.6980516626374353, -12.476481020835202], loss 8.716602071285436\n",
      "iter 1100, point [1.7368159644039771, -12.04715001603925], loss 7.969442965176621\n",
      "iter 1150, point [1.7375034995020395, -11.659343238414994], loss 7.365228465612388\n",
      "iter 1200, point [1.7085012931271857, -11.306424818680442], loss 6.861819342703047\n",
      "iter 1250, point [1.6565405824483015, -10.982995030930885], loss 6.431280353078019\n",
      "iter 1300, point [1.5870180647823104, -10.684652890749808], loss 6.054953198278096\n",
      "iter 1350, point [1.5042550040699705, -10.407804594738165], loss 5.720248083137862\n",
      "iter 1400, point [1.4117062100403601, -10.14950894127009], loss 5.418553777303124\n",
      "iter 1450, point [1.3121285818148223, -9.907352585055445], loss 5.143875665274019\n",
      "iter 1500, point [1.2077170340724794, -9.67934935975478], loss 4.891947653805328\n",
      "iter 1550, point [1.1002141124777076, -9.463859017459276], loss 4.659652555766873\n",
      "iter 1600, point [0.990998385834045, -9.259521632951046], loss 4.444643323159747\n",
      "iter 1650, point [0.8811557188942747, -9.065204645952335], loss 4.245095084874306\n",
      "iter 1700, point [0.7715367363576023, -8.87996009965401], loss 4.059542401818773\n",
      "iter 1750, point [0.662803148565214, -8.702990105791185], loss 3.88677206759292\n",
      "iter 1800, point [0.5554650931141796, -8.533618947271485], loss 3.7257521401326525\n",
      "iter 1850, point [0.4499112301277286, -8.371270536496699], loss 3.5755846299900256\n",
      "iter 1900, point [0.3464329929523944, -8.215450195281456], loss 3.435473657404253\n",
      "iter 1950, point [0.24524412503452966, -8.065729922139326], loss 3.3047037451160453\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights,1)\n",
    "        self.w[5] = -100.\n",
    "        self.w[9] = -100.\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w5, gradient_w9, eta=0.01):\n",
    "        net.w[5] = net.w[5] - eta * gradient_w5\n",
    "        net.w[9] = net.w[9] - eta * gradient_w9\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        points = []\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            points.append([net.w[5][0], net.w[9][0]])\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            gradient_w5 = gradient_w[5][0]\n",
    "            gradient_w9 = gradient_w[9][0]\n",
    "            self.update(gradient_w5, gradient_w9, eta)\n",
    "            losses.append(L)\n",
    "            if i % 50 == 0:\n",
    "                print('iter {}, point {}, loss {}'.format(i, [net.w[5][0], net.w[9][0]], L))\n",
    "        return points, losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=2000\n",
    "# 启动训练\n",
    "points, losses = net.train(x, y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（6）训练扩展到全部参数**\n",
    "\n",
    "为了能给读者直观的感受，上面演示的梯度下降的过程仅包含$w_5$和$w_9$两个参数，但房价预测的完整模型，必须要对所有参数$w$和$b$进行求解。这需要将Network中的`update`和`train`函数进行修改。由于不再限定参与计算的参数（所有参数均参与计算），修改之后的代码反而更加简洁。实现逻辑：“前向计算输出、根据输出和真实值计算Loss、基于Loss和输入计算梯度、根据梯度更新参数值”四个部分反复执行，直到到损失函数最小。具体代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.102688Z",
     "iopub.status.busy": "2022-09-23T06:53:29.101895Z",
     "iopub.status.idle": "2022-09-23T06:53:29.472842Z",
     "shell.execute_reply": "2022-09-23T06:53:29.471821Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.102656Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9, loss 5.143394325795511\n",
      "iter 19, loss 3.097924194225988\n",
      "iter 29, loss 2.082241020617026\n",
      "iter 39, loss 1.5673801618157397\n",
      "iter 49, loss 1.296620473507743\n",
      "iter 59, loss 1.1453399043319765\n",
      "iter 69, loss 1.0530155717435201\n",
      "iter 79, loss 0.9902292156463153\n",
      "iter 89, loss 0.9426576903842502\n",
      "iter 99, loss 0.9033048096880774\n",
      "iter 109, loss 0.868732003041364\n",
      "iter 119, loss 0.837229250968144\n",
      "iter 129, loss 0.807927474161227\n",
      "iter 139, loss 0.7803677341465796\n",
      "iter 149, loss 0.7542920908532763\n",
      "iter 159, loss 0.7295420168915829\n",
      "iter 169, loss 0.7060090054240883\n",
      "iter 179, loss 0.6836105084697766\n",
      "iter 189, loss 0.6622781710179414\n",
      "iter 199, loss 0.6419520361168637\n",
      "iter 209, loss 0.6225776517869489\n",
      "iter 219, loss 0.6041045903195837\n",
      "iter 229, loss 0.5864856570315078\n",
      "iter 239, loss 0.5696764374763879\n",
      "iter 249, loss 0.5536350125932016\n",
      "iter 259, loss 0.5383217588525027\n",
      "iter 269, loss 0.5236991929680566\n",
      "iter 279, loss 0.5097318413761649\n",
      "iter 289, loss 0.4963861247069634\n",
      "iter 299, loss 0.48363025234390233\n",
      "iter 309, loss 0.4714341245401978\n",
      "iter 319, loss 0.45976924072044867\n",
      "iter 329, loss 0.44860861316591\n",
      "iter 339, loss 0.43792668556597936\n",
      "iter 349, loss 0.4276992560632111\n",
      "iter 359, loss 0.4179034044959738\n",
      "iter 369, loss 0.40851742358635523\n",
      "iter 379, loss 0.39952075384787633\n",
      "iter 389, loss 0.39089392200622347\n",
      "iter 399, loss 0.3826184827405131\n",
      "iter 409, loss 0.37467696356451247\n",
      "iter 419, loss 0.36705281267772816\n",
      "iter 429, loss 0.35973034962581096\n",
      "iter 439, loss 0.35269471861856694\n",
      "iter 449, loss 0.3459318443621334\n",
      "iter 459, loss 0.3394283902696658\n",
      "iter 469, loss 0.3331717189222164\n",
      "iter 479, loss 0.3271498546584252\n",
      "iter 489, loss 0.32135144817819605\n",
      "iter 499, loss 0.31576574305173283\n",
      "iter 509, loss 0.3103825440311681\n",
      "iter 519, loss 0.30519218706757245\n",
      "iter 529, loss 0.30018551094136725\n",
      "iter 539, loss 0.29535383041913843\n",
      "iter 549, loss 0.29068891085453674\n",
      "iter 559, loss 0.28618294415539336\n",
      "iter 569, loss 0.28182852604338504\n",
      "iter 579, loss 0.2776186345365534\n",
      "iter 589, loss 0.27354660958874766\n",
      "iter 599, loss 0.2696061338236152\n",
      "iter 609, loss 0.265791214304132\n",
      "iter 619, loss 0.262096165281848\n",
      "iter 629, loss 0.258515591873034\n",
      "iter 639, loss 0.25504437461176843\n",
      "iter 649, loss 0.2516776548326958\n",
      "iter 659, loss 0.24841082083874047\n",
      "iter 669, loss 0.24523949481147192\n",
      "iter 679, loss 0.2421595204240984\n",
      "iter 689, loss 0.23916695111922887\n",
      "iter 699, loss 0.23625803901558054\n",
      "iter 709, loss 0.2334292244097483\n",
      "iter 719, loss 0.23067712584097294\n",
      "iter 729, loss 0.22799853068858242\n",
      "iter 739, loss 0.22539038627340988\n",
      "iter 749, loss 0.22284979143604464\n",
      "iter 759, loss 0.22037398856623475\n",
      "iter 769, loss 0.2179603560591435\n",
      "iter 779, loss 0.2156064011754777\n",
      "iter 789, loss 0.2133097532837386\n",
      "iter 799, loss 0.2110681574640261\n",
      "iter 809, loss 0.2088794684539304\n",
      "iter 819, loss 0.20674164491810018\n",
      "iter 829, loss 0.20465274402406475\n",
      "iter 839, loss 0.20261091630783168\n",
      "iter 849, loss 0.20061440081366638\n",
      "iter 859, loss 0.1986615204933024\n",
      "iter 869, loss 0.19675067785062839\n",
      "iter 879, loss 0.19488035081864621\n",
      "iter 889, loss 0.19304908885621125\n",
      "iter 899, loss 0.19125550925273513\n",
      "iter 909, loss 0.1894982936296714\n",
      "iter 919, loss 0.18777618462820622\n",
      "iter 929, loss 0.18608798277314595\n",
      "iter 939, loss 0.18443254350353405\n",
      "iter 949, loss 0.18280877436103968\n",
      "iter 959, loss 0.18121563232764162\n",
      "iter 969, loss 0.1796521213045923\n",
      "iter 979, loss 0.1781172897250724\n",
      "iter 989, loss 0.17661022829336184\n",
      "iter 999, loss 0.17513006784373505\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            self.update(gradient_w, gradient_b, eta)\n",
    "            losses.append(L)\n",
    "            if (i+1) % 10 == 0:\n",
    "                print('iter {}, loss {}'.format(i, L))\n",
    "        return losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=1000\n",
    "# 启动训练\n",
    "losses = net.train(x,y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（7）随机梯度下降法（ Stochastic Gradient Descent）**\n",
    "\n",
    "在上述程序中，每次损失函数和梯度计算都是基于数据集中的全量数据。对于波士顿房价预测任务数据集而言，样本数比较少，只有404个。但在实际问题中，数据集往往非常大，如果每次都使用全量数据进行计算，效率非常低，通俗地说就是“杀鸡焉用牛刀”。由于参数每次只沿着梯度反方向更新一点点，因此方向并不需要那么精确。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数，这种方法被称作随机梯度下降法（Stochastic Gradient Descent，SGD），核心概念如下：\n",
    "\n",
    "* minibatch：每次迭代时抽取出来的一批数据被称为一个minibatch。\n",
    "* batch size：每个minibatch所包含的样本数目称为batch size。\n",
    "* Epoch：当程序迭代的时候，按minibatch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮训练，也叫一个Epoch（轮次）。启动训练时，可以将训练的轮数`num_epochs`和`batch_size`作为参数传入。\n",
    "\n",
    "下面结合程序介绍具体的实现过程，涉及到数据处理和训练过程两部分代码的修改。\n",
    "\n",
    "1）数据处理需要实现拆分数据批次和样本乱序（为了实现随机抽样的效果）两个功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.474806Z",
     "iopub.status.busy": "2022-09-23T06:53:29.474189Z",
     "iopub.status.idle": "2022-09-23T06:53:29.486154Z",
     "shell.execute_reply": "2022-09-23T06:53:29.485241Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.474764Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data中一共包含404条数据，如果batch_size=10，即取前0-9号样本作为第一个minibatch，命名train_data1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.488132Z",
     "iopub.status.busy": "2022-09-23T06:53:29.487693Z",
     "iopub.status.idle": "2022-09-23T06:53:29.493318Z",
     "shell.execute_reply": "2022-09-23T06:53:29.492496Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.488103Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1 = train_data[0:10]\n",
    "train_data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用train_data1的数据（0-9号样本）计算梯度并更新网络参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.495010Z",
     "iopub.status.busy": "2022-09-23T06:53:29.494619Z",
     "iopub.status.idle": "2022-09-23T06:53:29.502007Z",
     "shell.execute_reply": "2022-09-23T06:53:29.501086Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.494974Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.497480200683046]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(13)\n",
    "x = train_data1[:, :-1]\n",
    "y = train_data1[:, -1:]\n",
    "loss = net.train(x, y, iterations=1, eta=0.01)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再取出10-19号样本作为第二个minibatch，计算梯度并更新网络参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.504055Z",
     "iopub.status.busy": "2022-09-23T06:53:29.503542Z",
     "iopub.status.idle": "2022-09-23T06:53:29.511219Z",
     "shell.execute_reply": "2022-09-23T06:53:29.510222Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.504024Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.849682302465982]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data2 = train_data[10:20]\n",
    "x = train_data2[:, :-1]\n",
    "y = train_data2[:, -1:]\n",
    "loss = net.train(x, y, iterations=1, eta=0.01)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按此方法不断的取出新的minibatch，并逐渐更新网络参数。\n",
    "\n",
    "接下来，将train_data分成大小为batch size的多个minibatch，如下代码所示：将train_data分成 $\\frac{404}{10} + 1 = 41$ 个 mini_batch，其中前40个mini_batch，每个均含有10个样本，最后一个minibatch只含有4个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.512967Z",
     "iopub.status.busy": "2022-09-23T06:53:29.512512Z",
     "iopub.status.idle": "2022-09-23T06:53:29.520032Z",
     "shell.execute_reply": "2022-09-23T06:53:29.519193Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.512923Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of mini_batches is  41\n",
      "first mini_batch shape  (10, 14)\n",
      "last mini_batch shape  (4, 14)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n = len(train_data)\n",
    "mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "print('total number of mini_batches is ', len(mini_batches))\n",
    "print('first mini_batch shape ', mini_batches[0].shape)\n",
    "print('last mini_batch shape ', mini_batches[-1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，这里是按顺序读取minibatch，而SGD里面是随机抽取一部分样本代表总体。为了实现随机抽样的效果，我们先将train_data里面的样本顺序随机打乱，然后再抽取minibatch。随机打乱样本顺序，需要用到`np.random.shuffle`函数，下面先介绍它的用法。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "通过大量实验发现，模型对最后出现的数据印象更加深刻。训练数据导入后，越接近模型训练结束，最后几个批次数据对模型参数的影响越大。为了避免模型记忆影响训练效果，需要进行样本乱序操作。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.522046Z",
     "iopub.status.busy": "2022-09-23T06:53:29.521608Z",
     "iopub.status.idle": "2022-09-23T06:53:29.561494Z",
     "shell.execute_reply": "2022-09-23T06:53:29.560553Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.522018Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "after shuffle [ 7  2 11  3  8  6 12  1  4  5 10  9]\n"
     ]
    }
   ],
   "source": [
    "# 新建一个array\n",
    "a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "print('before shuffle', a)\n",
    "np.random.shuffle(a)\n",
    "print('after shuffle', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次运行上面的代码，可以发现每次执行`shuffle`函数后的数字顺序均不同。上面举的是一个1维数组乱序的案例，我们再观察下2维数组乱序后的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.563272Z",
     "iopub.status.busy": "2022-09-23T06:53:29.562904Z",
     "iopub.status.idle": "2022-09-23T06:53:29.569634Z",
     "shell.execute_reply": "2022-09-23T06:53:29.568715Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.563247Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "after shuffle\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 9 10]\n",
      " [11 12]\n",
      " [ 7  8]]\n"
     ]
    }
   ],
   "source": [
    "# 新建一个array\n",
    "a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "a = a.reshape([6, 2])\n",
    "print('before shuffle\\n', a)\n",
    "np.random.shuffle(a)\n",
    "print('after shuffle\\n', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察运行结果可发现，数组的元素在第0维被随机打乱，但第1维的顺序保持不变。例如数字2仍然紧挨在数字1的后面，数字8仍然紧挨在数字7的后面，而第二维的[3, 4]并不排在[1, 2]的后面。将这部分实现SGD算法的代码集成到Network类中的`train`函数中，最终的完整代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.571180Z",
     "iopub.status.busy": "2022-09-23T06:53:29.570933Z",
     "iopub.status.idle": "2022-09-23T06:53:29.584478Z",
     "shell.execute_reply": "2022-09-23T06:53:29.583523Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.571156Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# 打乱样本顺序\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# 将train_data分成多个minibatch\n",
    "batch_size = 10\n",
    "n = len(train_data)\n",
    "mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "\n",
    "# 依次使用每个mini_batch的数据\n",
    "for mini_batch in mini_batches:\n",
    "    x = mini_batch[:, :-1]\n",
    "    y = mini_batch[:, -1:]\n",
    "    loss = net.train(x, y, iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）训练过程代码修改。将每个随机抽取的minibatch数据输入到模型中用于参数训练。训练过程的核心是两层循环：\n",
    "\n",
    "* 第一层循环，代表样本集合要被训练遍历几次，称为“epoch”，代码如下：\n",
    "\n",
    "`for epoch_id in range(num_epochs):`\n",
    "\n",
    "* 第二层循环，代表每次遍历时，样本集合被拆分成的多个批次，需要全部执行训练，称为“iter (iteration)”，代码如下：\n",
    "\n",
    "`for iter_id,mini_batch in emumerate(mini_batches):`\n",
    "\n",
    "在两层循环的内部是经典的四步训练流程：前向计算->计算损失->计算梯度->更新参数，这与大家之前所学是一致的，代码如下：\n",
    "\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)  #前向计算\n",
    "                loss = self.loss(a, y)  #计算损失\n",
    "                gradient_w, gradient_b = self.gradient(x, y)  #计算梯度\n",
    "                self.update(gradient_w, gradient_b, eta)  #更新参数\n",
    "\n",
    "\n",
    "将两部分改写的代码集成到Network类中的`train`函数中，最终的实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T06:53:29.586209Z",
     "iopub.status.busy": "2022-09-23T06:53:29.585843Z",
     "iopub.status.idle": "2022-09-23T06:53:29.889147Z",
     "shell.execute_reply": "2022-09-23T06:53:29.887954Z",
     "shell.execute_reply.started": "2022-09-23T06:53:29.586181Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 1.0281\n",
      "Epoch   0 / iter   1, loss = 0.5048\n",
      "Epoch   0 / iter   2, loss = 0.6382\n",
      "Epoch   0 / iter   3, loss = 0.5168\n",
      "Epoch   0 / iter   4, loss = 0.1951\n",
      "Epoch   1 / iter   0, loss = 0.6281\n",
      "Epoch   1 / iter   1, loss = 0.4611\n",
      "Epoch   1 / iter   2, loss = 0.4520\n",
      "Epoch   1 / iter   3, loss = 0.3961\n",
      "Epoch   1 / iter   4, loss = 0.1381\n",
      "Epoch   2 / iter   0, loss = 0.5642\n",
      "Epoch   2 / iter   1, loss = 0.4250\n",
      "Epoch   2 / iter   2, loss = 0.4480\n",
      "Epoch   2 / iter   3, loss = 0.3881\n",
      "Epoch   2 / iter   4, loss = 0.1884\n",
      "Epoch   3 / iter   0, loss = 0.3921\n",
      "Epoch   3 / iter   1, loss = 0.5582\n",
      "Epoch   3 / iter   2, loss = 0.3759\n",
      "Epoch   3 / iter   3, loss = 0.3849\n",
      "Epoch   3 / iter   4, loss = 0.1425\n",
      "Epoch   4 / iter   0, loss = 0.3821\n",
      "Epoch   4 / iter   1, loss = 0.4382\n",
      "Epoch   4 / iter   2, loss = 0.3864\n",
      "Epoch   4 / iter   3, loss = 0.4314\n",
      "Epoch   4 / iter   4, loss = 0.0471\n",
      "Epoch   5 / iter   0, loss = 0.4264\n",
      "Epoch   5 / iter   1, loss = 0.3829\n",
      "Epoch   5 / iter   2, loss = 0.3179\n",
      "Epoch   5 / iter   3, loss = 0.4149\n",
      "Epoch   5 / iter   4, loss = 0.1581\n",
      "Epoch   6 / iter   0, loss = 0.3148\n",
      "Epoch   6 / iter   1, loss = 0.3532\n",
      "Epoch   6 / iter   2, loss = 0.4195\n",
      "Epoch   6 / iter   3, loss = 0.3272\n",
      "Epoch   6 / iter   4, loss = 1.2465\n",
      "Epoch   7 / iter   0, loss = 0.3166\n",
      "Epoch   7 / iter   1, loss = 0.2810\n",
      "Epoch   7 / iter   2, loss = 0.4126\n",
      "Epoch   7 / iter   3, loss = 0.3309\n",
      "Epoch   7 / iter   4, loss = 0.2255\n",
      "Epoch   8 / iter   0, loss = 0.2555\n",
      "Epoch   8 / iter   1, loss = 0.3678\n",
      "Epoch   8 / iter   2, loss = 0.3342\n",
      "Epoch   8 / iter   3, loss = 0.3806\n",
      "Epoch   8 / iter   4, loss = 0.0570\n",
      "Epoch   9 / iter   0, loss = 0.3532\n",
      "Epoch   9 / iter   1, loss = 0.3973\n",
      "Epoch   9 / iter   2, loss = 0.1945\n",
      "Epoch   9 / iter   3, loss = 0.2839\n",
      "Epoch   9 / iter   4, loss = 0.1604\n",
      "Epoch  10 / iter   0, loss = 0.3414\n",
      "Epoch  10 / iter   1, loss = 0.2774\n",
      "Epoch  10 / iter   2, loss = 0.3439\n",
      "Epoch  10 / iter   3, loss = 0.2103\n",
      "Epoch  10 / iter   4, loss = 0.0959\n",
      "Epoch  11 / iter   0, loss = 0.3004\n",
      "Epoch  11 / iter   1, loss = 0.2497\n",
      "Epoch  11 / iter   2, loss = 0.2827\n",
      "Epoch  11 / iter   3, loss = 0.2987\n",
      "Epoch  11 / iter   4, loss = 0.0316\n",
      "Epoch  12 / iter   0, loss = 0.2509\n",
      "Epoch  12 / iter   1, loss = 0.2535\n",
      "Epoch  12 / iter   2, loss = 0.2944\n",
      "Epoch  12 / iter   3, loss = 0.2889\n",
      "Epoch  12 / iter   4, loss = 0.0547\n",
      "Epoch  13 / iter   0, loss = 0.2792\n",
      "Epoch  13 / iter   1, loss = 0.2137\n",
      "Epoch  13 / iter   2, loss = 0.2427\n",
      "Epoch  13 / iter   3, loss = 0.2986\n",
      "Epoch  13 / iter   4, loss = 0.3861\n",
      "Epoch  14 / iter   0, loss = 0.3261\n",
      "Epoch  14 / iter   1, loss = 0.2123\n",
      "Epoch  14 / iter   2, loss = 0.1837\n",
      "Epoch  14 / iter   3, loss = 0.2968\n",
      "Epoch  14 / iter   4, loss = 0.0620\n",
      "Epoch  15 / iter   0, loss = 0.2402\n",
      "Epoch  15 / iter   1, loss = 0.2823\n",
      "Epoch  15 / iter   2, loss = 0.2574\n",
      "Epoch  15 / iter   3, loss = 0.1833\n",
      "Epoch  15 / iter   4, loss = 0.0637\n",
      "Epoch  16 / iter   0, loss = 0.1889\n",
      "Epoch  16 / iter   1, loss = 0.1998\n",
      "Epoch  16 / iter   2, loss = 0.2031\n",
      "Epoch  16 / iter   3, loss = 0.3219\n",
      "Epoch  16 / iter   4, loss = 0.1373\n",
      "Epoch  17 / iter   0, loss = 0.2042\n",
      "Epoch  17 / iter   1, loss = 0.2070\n",
      "Epoch  17 / iter   2, loss = 0.2651\n",
      "Epoch  17 / iter   3, loss = 0.2137\n",
      "Epoch  17 / iter   4, loss = 0.0138\n",
      "Epoch  18 / iter   0, loss = 0.1794\n",
      "Epoch  18 / iter   1, loss = 0.1575\n",
      "Epoch  18 / iter   2, loss = 0.2554\n",
      "Epoch  18 / iter   3, loss = 0.2531\n",
      "Epoch  18 / iter   4, loss = 0.2192\n",
      "Epoch  19 / iter   0, loss = 0.1779\n",
      "Epoch  19 / iter   1, loss = 0.2072\n",
      "Epoch  19 / iter   2, loss = 0.2140\n",
      "Epoch  19 / iter   3, loss = 0.2513\n",
      "Epoch  19 / iter   4, loss = 0.0673\n",
      "Epoch  20 / iter   0, loss = 0.1634\n",
      "Epoch  20 / iter   1, loss = 0.1887\n",
      "Epoch  20 / iter   2, loss = 0.2515\n",
      "Epoch  20 / iter   3, loss = 0.1924\n",
      "Epoch  20 / iter   4, loss = 0.0926\n",
      "Epoch  21 / iter   0, loss = 0.1583\n",
      "Epoch  21 / iter   1, loss = 0.2319\n",
      "Epoch  21 / iter   2, loss = 0.1550\n",
      "Epoch  21 / iter   3, loss = 0.2092\n",
      "Epoch  21 / iter   4, loss = 0.1959\n",
      "Epoch  22 / iter   0, loss = 0.2414\n",
      "Epoch  22 / iter   1, loss = 0.1522\n",
      "Epoch  22 / iter   2, loss = 0.1719\n",
      "Epoch  22 / iter   3, loss = 0.1829\n",
      "Epoch  22 / iter   4, loss = 0.2748\n",
      "Epoch  23 / iter   0, loss = 0.1861\n",
      "Epoch  23 / iter   1, loss = 0.1830\n",
      "Epoch  23 / iter   2, loss = 0.1606\n",
      "Epoch  23 / iter   3, loss = 0.2351\n",
      "Epoch  23 / iter   4, loss = 0.1479\n",
      "Epoch  24 / iter   0, loss = 0.1678\n",
      "Epoch  24 / iter   1, loss = 0.2080\n",
      "Epoch  24 / iter   2, loss = 0.1471\n",
      "Epoch  24 / iter   3, loss = 0.1747\n",
      "Epoch  24 / iter   4, loss = 0.1607\n",
      "Epoch  25 / iter   0, loss = 0.1162\n",
      "Epoch  25 / iter   1, loss = 0.2067\n",
      "Epoch  25 / iter   2, loss = 0.1692\n",
      "Epoch  25 / iter   3, loss = 0.1757\n",
      "Epoch  25 / iter   4, loss = 0.0125\n",
      "Epoch  26 / iter   0, loss = 0.1707\n",
      "Epoch  26 / iter   1, loss = 0.1898\n",
      "Epoch  26 / iter   2, loss = 0.1409\n",
      "Epoch  26 / iter   3, loss = 0.1501\n",
      "Epoch  26 / iter   4, loss = 0.1002\n",
      "Epoch  27 / iter   0, loss = 0.1590\n",
      "Epoch  27 / iter   1, loss = 0.1801\n",
      "Epoch  27 / iter   2, loss = 0.1578\n",
      "Epoch  27 / iter   3, loss = 0.1257\n",
      "Epoch  27 / iter   4, loss = 0.7750\n",
      "Epoch  28 / iter   0, loss = 0.1573\n",
      "Epoch  28 / iter   1, loss = 0.1224\n",
      "Epoch  28 / iter   2, loss = 0.1353\n",
      "Epoch  28 / iter   3, loss = 0.1862\n",
      "Epoch  28 / iter   4, loss = 0.5305\n",
      "Epoch  29 / iter   0, loss = 0.1981\n",
      "Epoch  29 / iter   1, loss = 0.1114\n",
      "Epoch  29 / iter   2, loss = 0.1414\n",
      "Epoch  29 / iter   3, loss = 0.1856\n",
      "Epoch  29 / iter   4, loss = 0.0268\n",
      "Epoch  30 / iter   0, loss = 0.0984\n",
      "Epoch  30 / iter   1, loss = 0.1528\n",
      "Epoch  30 / iter   2, loss = 0.1637\n",
      "Epoch  30 / iter   3, loss = 0.1532\n",
      "Epoch  30 / iter   4, loss = 0.0846\n",
      "Epoch  31 / iter   0, loss = 0.1433\n",
      "Epoch  31 / iter   1, loss = 0.1643\n",
      "Epoch  31 / iter   2, loss = 0.1202\n",
      "Epoch  31 / iter   3, loss = 0.1215\n",
      "Epoch  31 / iter   4, loss = 0.2182\n",
      "Epoch  32 / iter   0, loss = 0.1567\n",
      "Epoch  32 / iter   1, loss = 0.1420\n",
      "Epoch  32 / iter   2, loss = 0.1073\n",
      "Epoch  32 / iter   3, loss = 0.1496\n",
      "Epoch  32 / iter   4, loss = 0.0846\n",
      "Epoch  33 / iter   0, loss = 0.1420\n",
      "Epoch  33 / iter   1, loss = 0.1369\n",
      "Epoch  33 / iter   2, loss = 0.0962\n",
      "Epoch  33 / iter   3, loss = 0.1480\n",
      "Epoch  33 / iter   4, loss = 0.0687\n",
      "Epoch  34 / iter   0, loss = 0.1234\n",
      "Epoch  34 / iter   1, loss = 0.1028\n",
      "Epoch  34 / iter   2, loss = 0.1407\n",
      "Epoch  34 / iter   3, loss = 0.1528\n",
      "Epoch  34 / iter   4, loss = 0.0390\n",
      "Epoch  35 / iter   0, loss = 0.1113\n",
      "Epoch  35 / iter   1, loss = 0.1289\n",
      "Epoch  35 / iter   2, loss = 0.1733\n",
      "Epoch  35 / iter   3, loss = 0.0892\n",
      "Epoch  35 / iter   4, loss = 0.0456\n",
      "Epoch  36 / iter   0, loss = 0.1358\n",
      "Epoch  36 / iter   1, loss = 0.0782\n",
      "Epoch  36 / iter   2, loss = 0.1475\n",
      "Epoch  36 / iter   3, loss = 0.1294\n",
      "Epoch  36 / iter   4, loss = 0.0442\n",
      "Epoch  37 / iter   0, loss = 0.1136\n",
      "Epoch  37 / iter   1, loss = 0.0954\n",
      "Epoch  37 / iter   2, loss = 0.1542\n",
      "Epoch  37 / iter   3, loss = 0.1262\n",
      "Epoch  37 / iter   4, loss = 0.0452\n",
      "Epoch  38 / iter   0, loss = 0.1277\n",
      "Epoch  38 / iter   1, loss = 0.1361\n",
      "Epoch  38 / iter   2, loss = 0.1103\n",
      "Epoch  38 / iter   3, loss = 0.0920\n",
      "Epoch  38 / iter   4, loss = 0.4119\n",
      "Epoch  39 / iter   0, loss = 0.1054\n",
      "Epoch  39 / iter   1, loss = 0.1165\n",
      "Epoch  39 / iter   2, loss = 0.1334\n",
      "Epoch  39 / iter   3, loss = 0.1240\n",
      "Epoch  39 / iter   4, loss = 0.0672\n",
      "Epoch  40 / iter   0, loss = 0.1218\n",
      "Epoch  40 / iter   1, loss = 0.0982\n",
      "Epoch  40 / iter   2, loss = 0.1077\n",
      "Epoch  40 / iter   3, loss = 0.1062\n",
      "Epoch  40 / iter   4, loss = 0.4781\n",
      "Epoch  41 / iter   0, loss = 0.1541\n",
      "Epoch  41 / iter   1, loss = 0.1049\n",
      "Epoch  41 / iter   2, loss = 0.0979\n",
      "Epoch  41 / iter   3, loss = 0.1042\n",
      "Epoch  41 / iter   4, loss = 0.0397\n",
      "Epoch  42 / iter   0, loss = 0.0996\n",
      "Epoch  42 / iter   1, loss = 0.1031\n",
      "Epoch  42 / iter   2, loss = 0.1294\n",
      "Epoch  42 / iter   3, loss = 0.0980\n",
      "Epoch  42 / iter   4, loss = 0.1135\n",
      "Epoch  43 / iter   0, loss = 0.1521\n",
      "Epoch  43 / iter   1, loss = 0.1088\n",
      "Epoch  43 / iter   2, loss = 0.1089\n",
      "Epoch  43 / iter   3, loss = 0.0775\n",
      "Epoch  43 / iter   4, loss = 0.1444\n",
      "Epoch  44 / iter   0, loss = 0.0827\n",
      "Epoch  44 / iter   1, loss = 0.0875\n",
      "Epoch  44 / iter   2, loss = 0.1428\n",
      "Epoch  44 / iter   3, loss = 0.1002\n",
      "Epoch  44 / iter   4, loss = 0.0352\n",
      "Epoch  45 / iter   0, loss = 0.0917\n",
      "Epoch  45 / iter   1, loss = 0.1193\n",
      "Epoch  45 / iter   2, loss = 0.0933\n",
      "Epoch  45 / iter   3, loss = 0.1044\n",
      "Epoch  45 / iter   4, loss = 0.0064\n",
      "Epoch  46 / iter   0, loss = 0.1020\n",
      "Epoch  46 / iter   1, loss = 0.0913\n",
      "Epoch  46 / iter   2, loss = 0.0882\n",
      "Epoch  46 / iter   3, loss = 0.1170\n",
      "Epoch  46 / iter   4, loss = 0.0330\n",
      "Epoch  47 / iter   0, loss = 0.0696\n",
      "Epoch  47 / iter   1, loss = 0.0996\n",
      "Epoch  47 / iter   2, loss = 0.0948\n",
      "Epoch  47 / iter   3, loss = 0.1109\n",
      "Epoch  47 / iter   4, loss = 0.5095\n",
      "Epoch  48 / iter   0, loss = 0.0929\n",
      "Epoch  48 / iter   1, loss = 0.1220\n",
      "Epoch  48 / iter   2, loss = 0.1150\n",
      "Epoch  48 / iter   3, loss = 0.0917\n",
      "Epoch  48 / iter   4, loss = 0.0968\n",
      "Epoch  49 / iter   0, loss = 0.0732\n",
      "Epoch  49 / iter   1, loss = 0.0808\n",
      "Epoch  49 / iter   2, loss = 0.0896\n",
      "Epoch  49 / iter   3, loss = 0.1306\n",
      "Epoch  49 / iter   4, loss = 0.1896\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        #np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        N = x.shape[0]\n",
    "        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = 1. / N * np.sum(z-y)\n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "            \n",
    "                \n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epochs):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                #print(self.w.shape)\n",
    "                #print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)\n",
    "                loss = self.loss(a, y)\n",
    "                gradient_w, gradient_b = self.gradient(x, y)\n",
    "                self.update(gradient_w, gradient_b, eta)\n",
    "                losses.append(loss)\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                                 format(epoch_id, iter_id, loss))\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察上述损失函数的变化，随机梯度下降加快了训练过程，但由于每次仅基于少量样本更新参数和计算损失ii，所以损失下降曲线会出现震荡。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "由于房价预测的数据量过少，所以难以感受到随机梯度下降带来的性能提升。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本节我们详细介绍了如何使用Numpy实现梯度下降算法，构建并训练了一个简单的线性模型实现波士顿房价预测，可以总结出，使用神经网络建模房价预测有三个要点：\n",
    "\n",
    "* 构建网络，初始化参数$w$和$b$，定义预测和损失函数的计算方法。\n",
    "* 随机选择初始点，建立梯度的计算方法和参数更新方式。\n",
    "* 将数据集的数据按batch size的大小分成多个minibatch，分别灌入模型计算梯度并更新参数，不断迭代直到损失函数几乎不再下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业1-2\n",
    "\n",
    "1. 样本归一化：预测时的样本数据同样也需要归一化，但使用训练样本的均值和极值计算，这是为什么？\n",
    "\n",
    "2. 当部分参数的梯度计算为0（接近0）时，可能是什么情况？是否意味着完成训练？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业 1-3\n",
    "\n",
    "1. 随机梯度下降的batchsize设置成多少合适？过小有什么问题？过大有什么问题？提示：过大以整个样本集合为例，过小以单个样本为例来思考。\n",
    "1. 一次训练使用的配置：5个epoch，1000个样本，batchsize=20，最内层循环执行多少轮？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业1-4\n",
    "\n",
    "#### 基本知识\n",
    "\n",
    "**1. 求导的链式法则**\n",
    "\n",
    "链式法则是微积分中的求导法则，用于求一个复合函数的导数，是在微积分的求导运算中一种常用的方法。复合函数的导数将是构成复合这有限个函数在相应点的导数的乘积，就像锁链一样一环套一环，故称链式法则。如 **图9** 所示，如果求最终输出对内层输入（第一层）的梯度，等于外层梯度（第二层）乘以本层函数的梯度。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2beffa3f3d7c402685671b0825561a91c17216fe8b924f64b9f29a96f45cbc85\" width=\"200\" hegiht=\"\" ></center>\n",
    "<center><br>图9：求导的链式法则</br></center>\n",
    "<br></br>\n",
    "\n",
    "**2. 计算图的概念**\n",
    "\n",
    "（1）为何是反向计算梯度？即梯度是由网络后端向前端计算。当前层的梯度要依据处于网络中后一层的梯度来计算，所以只有先算后一层的梯度才能计算本层的梯度。     \n",
    "\n",
    "（2）案例：购买苹果产生消费的计算图。假设一家商店9折促销苹果，每个的单价100元。计算一个顾客总消费的结构如 **图10** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/46c43ead4fa942f5be87f25538a046ff9456516816274cbcb5f6df3768c0fd34\" width=\"400\" hegiht=\"40\" ></center>\n",
    "<center><br>图10：购买苹果所产生的消费计算图</br></center>\n",
    "<br></br>\n",
    "\n",
    "*  前向计算过程：以黑色箭头表示，顾客购买了2个苹果，再加上九折的折扣，一共消费100\\*2\\*0.9=180元。\n",
    "*  后向传播过程：以红色箭头表示，根据链式法则，本层的梯度计算 * 后一层传递过来的梯度，所以需从后向前计算。\n",
    " \n",
    "最后一层的输出对自身的求导为1。导数第二层根据 **图11** 所示的乘法求导的公式，分别为0.9\\*1和200\\*1。同样的，第三层为100 * 0.9=90，2 * 0.9=1.8。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c251a2c290e946f99ce3a3381396c392b50e5a4243c346509bd91177b7f2da90\" width=\"200\"  ></center>\n",
    "<center><br>图11：乘法求导的公式</br></center>\n",
    "<br></br>\n",
    "\n",
    "#### 作业题\n",
    "\n",
    "1. 根据 **图12** 所示的乘法和加法的导数公式，完成 **图13** 购买苹果和橘子的梯度传播的题目。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4ce8715f03f9477699707056544b1e6363f78aa09fda411d972878abb6d1d26f\" width=\"300\"  ></center>\n",
    "<center><br>图12：乘法和加法的导数公式</br></center>\n",
    "<br></br>\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2fc6665e10f34f9e863172bb399862319f0914467d72457d9e7328616bdbe6df\" width=\"500\"  ></center>\n",
    "<center><br>图13：购买苹果和橘子产生消费的计算图</br></center>\n",
    "<br></br>  \n",
    "\n",
    "2. 挑战题：用代码实现两层的神经网络的梯度传播，中间层的尺寸为13【房价预测案例】（教案当前的版本为一层的神经网络），如 **图14** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/580f2553aa4643809006f5a8d3deb2aa8dd4e1aa69d94cf6a35ead5fe7cf469e\" width=\"300\"  ></center>\n",
    "<center><br>图14：两层的神经网络</br></center>\n",
    "<br></br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
